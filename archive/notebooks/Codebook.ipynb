{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1574013-6540-4864-8c5e-eeb075d92f45",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Split the codebook on training and validation parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea227ca-2e92-43c3-8d25-abed8f0b3063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Lambda\n",
    "from keras.layers import LSTM, Embedding, Bidirectional, BatchNormalization, SimpleRNN, Input, TimeDistributed\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.keras import TqdmCallback\n",
    "from scipy.spatial import distance\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b61b9ff3-ed2d-4938-a2d4-dd3075e29d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 8                       # number of information bits\n",
    "N = 16                      # code length\n",
    "\n",
    "nb_epoch = 2**18 \n",
    "batch_size = 256            # size of batches for calculation the gradient\n",
    "LLR = False                 # 'True' enables the log-likelihood-ratio layer\n",
    "noise = True\n",
    "optimizer = 'adam'           \n",
    "loss = 'mse'                # or 'binary_crossentropy'\n",
    "\n",
    "train_SNR_Eb = 1            # training-Eb/No\n",
    "train_SNR_Es = train_SNR_Eb + 10*np.log10(k/N)\n",
    "train_sigma = np.sqrt(1/(2*10**(train_SNR_Es/10))).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd18112a-8799-4229-8513-cdd90096dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulateBPSK(x):\n",
    "    return -2*x +1;\n",
    "\n",
    "def addNoise(x, sigma):\n",
    "    w = K.random_normal(K.shape(x), mean=0.0, stddev=sigma)        # std was changed to stddev \n",
    "    return x + w\n",
    "\n",
    "def ber(y_true, y_pred):\n",
    "    return K.mean(K.not_equal(y_true, K.round(y_pred)))    \n",
    "\n",
    "def return_output_shape(input_shape):  \n",
    "    return input_shape\n",
    "\n",
    "def compose_model(layers):\n",
    "    model = Sequential()\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "    return model\n",
    "\n",
    "def log_likelihood_ratio(x, sigma):   \n",
    "    return 2*x/np.float32(sigma**2)\n",
    "\n",
    "def errors(y_true, y_pred):\n",
    "    return K.sum(int(K.not_equal(y_true, K.round(y_pred))))\n",
    "\n",
    "def reshape_codebook(cb):\n",
    "    global N\n",
    "    global output_shape_test\n",
    "    output_shape_test = tf.reshape(cb, (-1, N, 1))\n",
    "    return output_shape_test\n",
    "\n",
    "output_shape_test = (0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f96e5b-dd6d-4890-904e-0df0c99613fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(current_arch):\n",
    "    # Define modulator\n",
    "    modulator_layers = [Lambda(modulateBPSK, \n",
    "                              input_shape=(N,), output_shape=return_output_shape, name=\"modulator\")]\n",
    "    modulator = compose_model(modulator_layers)\n",
    "    modulator.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    # Define noise\n",
    "    noise_layers = [Lambda(addNoise, arguments={'sigma':train_sigma}, \n",
    "                           input_shape=(N,), output_shape=return_output_shape, name=\"noise\")]\n",
    "    noise = compose_model(noise_layers)\n",
    "    noise.compile(optimizer=optimizer, loss=loss)\n",
    "    \n",
    "    # Reshape layer\n",
    "    reshape_layer = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape')]\n",
    "    reshape = compose_model(reshape_layer)\n",
    "    reshape.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    # Define LLR\n",
    "    llr_layers = [Lambda(log_likelihood_ratio, arguments={'sigma':train_sigma}, \n",
    "                         input_shape=(N,), output_shape=return_output_shape, name=\"LLR\")]\n",
    "    llr = compose_model(llr_layers)\n",
    "    llr.compile(optimizer=optimizer, loss=loss)\n",
    "    \n",
    "    \n",
    "    # Define decoder\n",
    "    \n",
    "    if current_arch == 'dense-128-64-32':\n",
    "        decoder_layers = [Dense(128, activation='relu'),\n",
    "                          Dense(64, activation='relu'),\n",
    "                          Dense(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'dense-256-128-64-32':\n",
    "        decoder_layers = [Dense(256, activation='relu', input_shape=(N,)),\n",
    "                          Dense(128, activation='relu'),\n",
    "                          Dense(64, activation='relu'),\n",
    "                          Dense(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'lstm-32':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'lstm-64':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(64, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'lstm-128':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(128, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'lstm-256':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(256, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "        \n",
    "    elif current_arch == 'lstm-64-32':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(64, activation='relu', return_sequences=True), \n",
    "                          LSTM(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "        \n",
    "    elif current_arch == 'lstm-128-64-32':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(128, activation='relu', return_sequences=True), \n",
    "                          LSTM(64, activation='relu', return_sequences=True), \n",
    "                          LSTM(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'lstm-256-128-64-32':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(256, activation='relu', return_sequences=True),\n",
    "                          LSTM(128, activation='relu', return_sequences=True),\n",
    "                          LSTM(64, activation='relu', return_sequences=True), \n",
    "                          LSTM(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "\n",
    "\n",
    "    # Define model\n",
    "#     if (LLR and noise):\n",
    "#         model_layers = modulator_layers + noise_layers  + llr_layers + decoder_layers\n",
    "#     elif (not LLR and noise):\n",
    "#         model_layers = modulator_layers + noise_layers + decoder_layers\n",
    "#     elif (not LLR and not noise):\n",
    "#         model_layers = modulator_layers + decoder_layers\n",
    "\n",
    "    if noise:\n",
    "        model_layers = modulator_layers + noise_layers + decoder_layers\n",
    "    else:\n",
    "        model_layers = modulator_layers + decoder_layers\n",
    "    model = compose_model(model_layers)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[ber])\n",
    "    return model, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e80a74c4-bd24-496e-a5c6-f046c277aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_adder(a,b):\n",
    "    s = a ^ b\n",
    "    c = a & b\n",
    "    return s,c\n",
    "\n",
    "def full_adder(a,b,c):\n",
    "    s = (a ^ b) ^ c\n",
    "    c = (a & b) | (c & (a ^ b))\n",
    "    return s,c\n",
    "\n",
    "def add_bool(a,b):\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError('arrays with different length')\n",
    "    k = len(a)\n",
    "    s = np.zeros(k,dtype=bool)\n",
    "    c = False\n",
    "    for i in reversed(range(0,k)):\n",
    "        s[i], c = full_adder(a[i],b[i],c)    \n",
    "    if c:\n",
    "        warnings.warn(\"Addition overflow!\")\n",
    "    return s\n",
    "\n",
    "def inc_bool(a):\n",
    "    k = len(a)\n",
    "    increment = np.hstack((np.zeros(k-1,dtype=bool), np.ones(1,dtype=bool)))\n",
    "    a = add_bool(a,increment)\n",
    "    return a\n",
    "\n",
    "def bitrevorder(x):\n",
    "    m = np.amax(x)\n",
    "    n = np.ceil(np.log2(m)).astype(int)\n",
    "    for i in range(0,len(x)):\n",
    "        x[i] = int('{:0{n}b}'.format(x[i],n=n)[::-1],2)  \n",
    "    return x\n",
    "\n",
    "def int2bin(x,N):\n",
    "    if isinstance(x, list) or isinstance(x, np.ndarray):\n",
    "        binary = np.zeros((len(x),N),dtype='bool')\n",
    "        for i in range(0,len(x)):\n",
    "            binary[i] = np.array([int(j) for j in bin(x[i])[2:].zfill(N)])\n",
    "    else:\n",
    "        binary = np.array([int(j) for j in bin(x)[2:].zfill(N)],dtype=bool)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def bin2int(b):\n",
    "    if isinstance(b[0], list):\n",
    "        integer = np.zeros((len(b),),dtype=int)\n",
    "        for i in range(0,len(b)):\n",
    "            out = 0\n",
    "            for bit in b[i]:\n",
    "                out = (out << 1) | bit\n",
    "            integer[i] = out\n",
    "    elif isinstance(b, np.ndarray):\n",
    "        if len(b.shape) == 1:\n",
    "            out = 0\n",
    "            for bit in b:\n",
    "                out = (out << 1) | bit\n",
    "            integer = out     \n",
    "        else:\n",
    "            integer = np.zeros((b.shape[0],),dtype=int)\n",
    "            for i in range(0,b.shape[0]):\n",
    "                out = 0\n",
    "                for bit in b[i]:\n",
    "                    out = (out << 1) | bit\n",
    "                integer[i] = out\n",
    "        \n",
    "    return integer\n",
    "\n",
    "def polar_design_awgn(N, k, design_snr_dB):  \n",
    "        \n",
    "    S = 10**(design_snr_dB/10)\n",
    "    z0 = np.zeros(N)\n",
    "\n",
    "    z0[0] = np.exp(-S)\n",
    "    for j in range(1,int(np.log2(N))+1):\n",
    "        u = 2**j\n",
    "        for t in range(0,int(u/2)):\n",
    "            T = z0[t]\n",
    "            z0[t] = 2*T - T**2     # upper channel\n",
    "            z0[int(u/2)+t] = T**2  # lower channel\n",
    "        \n",
    "    # sort into increasing order\n",
    "    idx = np.argsort(z0)\n",
    "        \n",
    "    # select k best channels\n",
    "    idx = np.sort(bitrevorder(idx[0:k]))\n",
    "    \n",
    "    A = np.zeros(N, dtype=bool)\n",
    "    A[idx] = True\n",
    "        \n",
    "    return A\n",
    "\n",
    "def polar_transform_iter(u):\n",
    "\n",
    "    N = len(u)\n",
    "    n = 1\n",
    "    x = np.copy(u)\n",
    "    stages = np.log2(N).astype(int)\n",
    "    for s in range(0,stages):\n",
    "        i = 0\n",
    "        while i < N:\n",
    "            for j in range(0,n):\n",
    "                idx = i+j\n",
    "                x[idx] = x[idx] ^ x[idx+n]\n",
    "            i=i+2*n\n",
    "        n=2*n\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15b45e9-cbb2-444a-9353-f91592f4f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_words(code):\n",
    "    # Create all possible information words\n",
    "    d = np.zeros((2**k,k),dtype=bool)\n",
    "    for i in range(1,2**k):\n",
    "        d[i]= inc_bool(d[i-1])\n",
    "\n",
    "    # Create sets of all possible codewords (codebook)\n",
    "    if code == 'polar':   \n",
    "\n",
    "        A = polar_design_awgn(N, k, design_snr_dB=0)  # logical vector indicating the nonfrozen bit locations \n",
    "        x = np.zeros((2**k, N),dtype=bool)\n",
    "        u = np.zeros((2**k, N),dtype=bool)\n",
    "        u[:,A] = d\n",
    "\n",
    "        for i in range(0,2**k):\n",
    "            x[i] = polar_transform_iter(u[i])\n",
    "        return x, d, A\n",
    "\n",
    "    elif code == 'random':\n",
    "\n",
    "        np.random.seed(4267)   # for a 16bit Random Code (r=0.5) with Hamming distance >= 2\n",
    "        x = np.random.randint(0,2,size=(2**k,N), dtype=bool)\n",
    "        return x, d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ca601-a4b5-4b74-aae5-c7a300c85fbe",
   "metadata": {},
   "source": [
    "### Create codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79694042-4e7d-4ae0-aad0-e2401c8402d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = 'polar'              # type of code ('random' or 'polar')\n",
    "codewords, inputs, log_vector = create_words(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cdff52-a223-47ce-9db9-5fd6c4513497",
   "metadata": {},
   "source": [
    "# With noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2878d216-2883-449b-a3cd-7d84c381e882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387709f5-90fc-4626-a282-1ce84a654b7d",
   "metadata": {},
   "source": [
    "## i*train_size+i, train_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0048332-6b83-4c57-ba8b-2901cd66eb56",
   "metadata": {},
   "source": [
    "### Sptit codebook on train / validation parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd0317a8-ea0e-45f8-a0de-9b7122488a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training indices: [0, 17, 34, 51, 68, 85, 102, 119, 136, 153, 170, 187, 204, 221, 238, 255]\n"
     ]
    }
   ],
   "source": [
    "train_size = 16\n",
    "idx = []\n",
    "for i in range(train_size):\n",
    "    idx.append(i*train_size+i)\n",
    "print(f'training indices: {idx}')\n",
    "inputs_train = inputs[idx]\n",
    "codewords_train = codewords[idx]\n",
    "d_test = np.delete(inputs, idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83cdc40e-fc46-4f94-8b06-ae090da9d453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 1],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 1, 1, 0, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 1, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 1, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 1, 1, 0, 1, 1],\n",
       "       [1, 1, 0, 0, 1, 1, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train.astype(int)[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff6ce60-3035-431c-8e54-a95f3b655d1f",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f334e38-c35b-4bfa-aa4b-f07e5d96c624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 18:44:50.134089: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-27 18:44:51.129711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10349 MB memory:  -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1\n",
      "2023-02-27 18:44:51.130506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10407 MB memory:  -> device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:b3:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5d699f5dad4daa9e2d1a181ecbf151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 18:44:51.703276: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model, decoder = update_model('dense-128-64-32')\n",
    "history = model.fit(codewords_train, inputs_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                    shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed0a6d-c4d7-4605-bb81-3a7b6f60fd65",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29edb85-12ed-4b3b-a09a-72ddfa72cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = 1000000\n",
    "\n",
    "SNR_dB_start_Eb = 0\n",
    "SNR_dB_stop_Eb = 10\n",
    "SNR_points = 20\n",
    "\n",
    "SNR_dB_start_Es = SNR_dB_start_Eb + 10*np.log10(k/N)\n",
    "SNR_dB_stop_Es = SNR_dB_stop_Eb + 10*np.log10(k/N)\n",
    "\n",
    "sigma_start = np.sqrt(1/(2*10**(SNR_dB_start_Es/10)))\n",
    "sigma_stop = np.sqrt(1/(2*10**(SNR_dB_stop_Es/10)))\n",
    "sigmas = np.linspace(sigma_start, sigma_stop, SNR_points)\n",
    "\n",
    "nb_errors = np.zeros((1,2**k,len(sigmas)),dtype=int)\n",
    "nb_bits = np.zeros((1,2**k,len(sigmas)),dtype=int)\n",
    "exp_descr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef0fe72-1a7f-4f19-b661-718a9afdf7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_descr.append('i*train_size+i')           # Add legend\n",
    "\n",
    "for i in range(0,len(sigmas)):\n",
    "\n",
    "    x_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "    u_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "\n",
    "    u_test[:, log_vector] = d_test\n",
    "    for ii in range(0,d_test.shape[0]):\n",
    "\n",
    "        print(f'Decoding SNR: {i+1}/{len(sigmas)}, CW: {ii+1}/{d_test.shape[0]}   ', end=\"\\r\")\n",
    "\n",
    "        x_test[ii] = polar_transform_iter(u_test[ii])\n",
    "        d_test_batch = np.tile(d_test[ii], (test_batch, 1))\n",
    "        x_test_batch = np.tile(x_test[ii], (test_batch, 1))\n",
    "\n",
    "\n",
    "        # Modulator (BPSK)\n",
    "        s_test = -2*x_test_batch + 1\n",
    "\n",
    "        # Channel (AWGN)\n",
    "        y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "\n",
    "        nb_bits[nb_bits.shape[0]-1][ii][i] += d_test_batch.size\n",
    "        nb_errors[nb_errors.shape[0]-1][ii][i] += decoder.evaluate(y_test, d_test_batch, batch_size=test_batch, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6129540-4201-4823-a9e0-92cf2c9e4e3d",
   "metadata": {},
   "source": [
    "## Take first 16 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdc35e1-94c8-4bdb-ba00-4bd0593c3aec",
   "metadata": {},
   "source": [
    "### Sptit codebook on train / validation parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4a769-c7f8-4317-b1dd-2d574f274e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 16\n",
    "idx = list(np.arange(train_size, dtype=int))\n",
    "print(f'training indices: {idx}')\n",
    "inputs_train = inputs[idx]\n",
    "codewords_train = codewords[idx]\n",
    "d_test = np.delete(inputs, idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8a759-3c29-493d-9eeb-8c381891d5de",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc4cfb4-5a78-415c-ae6c-df9ad8ca696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model, decoder = update_model('dense-128-64-32')\n",
    "history = model.fit(codewords_train, inputs_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                    shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f07c942-415e-4051-ad78-80d87de9f328",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fcf345-b574-4b6b-9229-8ed0b9f57acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_descr.append('first 16 elements')                                          # Add legend\n",
    "nb_errors = np.append(nb_errors, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "nb_bits = np.append(nb_bits, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "\n",
    "for i in range(0,len(sigmas)):\n",
    "\n",
    "    x_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "    u_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "\n",
    "    u_test[:, log_vector] = d_test\n",
    "    for ii in range(0,d_test.shape[0]):\n",
    "\n",
    "        print(f'Decoding SNR: {i+1}/{len(sigmas)}, CW: {ii+1}/{d_test.shape[0]}   ', end=\"\\r\")\n",
    "\n",
    "        x_test[ii] = polar_transform_iter(u_test[ii])\n",
    "        d_test_batch = np.tile(d_test[ii], (test_batch, 1))\n",
    "        x_test_batch = np.tile(x_test[ii], (test_batch, 1))\n",
    "\n",
    "\n",
    "        # Modulator (BPSK)\n",
    "        s_test = -2*x_test_batch + 1\n",
    "\n",
    "        # Channel (AWGN)\n",
    "        y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "\n",
    "        nb_bits[nb_bits.shape[0]-1][ii][i] += d_test_batch.size\n",
    "        nb_errors[nb_errors.shape[0]-1][ii][i] += decoder.evaluate(y_test, d_test_batch, batch_size=test_batch, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9e5e08-a826-4dfd-9962-37d84886df6f",
   "metadata": {},
   "source": [
    "## Take first 32 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7886ff6c-ef62-49cc-96eb-379d256c04ea",
   "metadata": {},
   "source": [
    "### Sptit codebook on train / validation parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d53f2-07ce-4a1c-bc3e-8eafb3de74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 32\n",
    "idx = list(np.arange(train_size, dtype=int))\n",
    "print(f'training indices: {idx}')\n",
    "inputs_train = inputs[idx]\n",
    "codewords_train = codewords[idx]\n",
    "d_test = np.delete(inputs, idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b57a7-becd-4518-b335-5695fa6a657e",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c6fb3b-1e5d-4502-b26f-aa29e14446a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model, decoder = update_model('dense-128-64-32')\n",
    "history = model.fit(codewords_train, inputs_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                    shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d84169-183a-47da-9e60-a49a9d65ddc7",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd70689-3aca-4b5e-a14f-98903ed88d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_descr.append('first 32 elements')                                          # Add legend\n",
    "nb_errors = np.append(nb_errors, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "nb_bits = np.append(nb_bits, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "\n",
    "for i in range(0,len(sigmas)):\n",
    "\n",
    "    x_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "    u_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "\n",
    "    u_test[:, log_vector] = d_test\n",
    "    for ii in range(0,d_test.shape[0]):\n",
    "\n",
    "        print(f'Decoding SNR: {i+1}/{len(sigmas)}, CW: {ii+1}/{d_test.shape[0]}   ', end=\"\\r\")\n",
    "\n",
    "        x_test[ii] = polar_transform_iter(u_test[ii])\n",
    "        d_test_batch = np.tile(d_test[ii], (test_batch, 1))\n",
    "        x_test_batch = np.tile(x_test[ii], (test_batch, 1))\n",
    "\n",
    "\n",
    "        # Modulator (BPSK)\n",
    "        s_test = -2*x_test_batch + 1\n",
    "\n",
    "        # Channel (AWGN)\n",
    "        y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "\n",
    "        nb_bits[nb_bits.shape[0]-1][ii][i] += d_test_batch.size\n",
    "        nb_errors[nb_errors.shape[0]-1][ii][i] += decoder.evaluate(y_test, d_test_batch, batch_size=test_batch, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8de6ae-9f22-429e-91de-66c881e6eafb",
   "metadata": {},
   "source": [
    "## Take first 64 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23efa8b6-b478-412f-ac3a-0fa53da586f1",
   "metadata": {},
   "source": [
    "### Sptit codebook on train / validation parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffc928a-63ab-4c8c-a9b0-5f3730c23596",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 64\n",
    "idx = list(np.arange(train_size, dtype=int))\n",
    "print(f'training indices: {idx}')\n",
    "inputs_train = inputs[idx]\n",
    "codewords_train = codewords[idx]\n",
    "d_test = np.delete(inputs, idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fa09c4-0f9e-4e41-a658-a32d35a73e65",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df50fca-302f-4f3e-bc0d-39ab2c6964f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model, decoder = update_model('dense-128-64-32')\n",
    "history = model.fit(codewords_train, inputs_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                    shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b32838-573f-4284-824c-cc6dc4a4aecf",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c5a7e4-0ba3-42ed-9996-9dd3b0cc2d22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_descr.append('first 64 elements')                                          # Add legend\n",
    "nb_errors = np.append(nb_errors, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "nb_bits = np.append(nb_bits, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "\n",
    "for i in range(0,len(sigmas)):\n",
    "\n",
    "    x_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "    u_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "\n",
    "    u_test[:, log_vector] = d_test\n",
    "    for ii in range(0,d_test.shape[0]):\n",
    "\n",
    "        print(f'Decoding SNR: {i+1}/{len(sigmas)}, CW: {ii+1}/{d_test.shape[0]}   ', end=\"\\r\")\n",
    "\n",
    "        x_test[ii] = polar_transform_iter(u_test[ii])\n",
    "        d_test_batch = np.tile(d_test[ii], (test_batch, 1))\n",
    "        x_test_batch = np.tile(x_test[ii], (test_batch, 1))\n",
    "\n",
    "\n",
    "        # Modulator (BPSK)\n",
    "        s_test = -2*x_test_batch + 1\n",
    "\n",
    "        # Channel (AWGN)\n",
    "        y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "\n",
    "        nb_bits[nb_bits.shape[0]-1][ii][i] += d_test_batch.size\n",
    "        nb_errors[nb_errors.shape[0]-1][ii][i] += decoder.evaluate(y_test, d_test_batch, batch_size=test_batch, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f055b6e-07a5-4131-a1df-1afe6b50f0ba",
   "metadata": {},
   "source": [
    "## Take first 128 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea00e3-f186-42a8-ad85-77886fb2ef37",
   "metadata": {},
   "source": [
    "### Sptit codebook on train / validation parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a376e9c-8e1c-4a25-a833-ff89d03960c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 128\n",
    "idx = list(np.arange(train_size, dtype=int))\n",
    "print(f'training indices: {idx}')\n",
    "inputs_train = inputs[idx]\n",
    "codewords_train = codewords[idx]\n",
    "d_test = np.delete(inputs, idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb68ec-1f1f-4192-b448-d159fba95e36",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80e94d-271b-4f55-994c-83679d284e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model, decoder = update_model('dense-128-64-32')\n",
    "history = model.fit(codewords_train, inputs_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                    shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4716efcb-19c6-45f4-81bb-b6b189b7a3f2",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c613e-34b2-4078-8848-33e6c315a382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_descr.append('first 128 elements')                                          # Add legend\n",
    "nb_errors = np.append(nb_errors, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "nb_bits = np.append(nb_bits, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "\n",
    "for i in range(0,len(sigmas)):\n",
    "\n",
    "    x_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "    u_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "\n",
    "    u_test[:, log_vector] = d_test\n",
    "    for ii in range(0,d_test.shape[0]):\n",
    "\n",
    "        print(f'Decoding SNR: {i+1}/{len(sigmas)}, CW: {ii+1}/{d_test.shape[0]}   ', end=\"\\r\")\n",
    "\n",
    "        x_test[ii] = polar_transform_iter(u_test[ii])\n",
    "        d_test_batch = np.tile(d_test[ii], (test_batch, 1))\n",
    "        x_test_batch = np.tile(x_test[ii], (test_batch, 1))\n",
    "\n",
    "\n",
    "        # Modulator (BPSK)\n",
    "        s_test = -2*x_test_batch + 1\n",
    "\n",
    "        # Channel (AWGN)\n",
    "        y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "\n",
    "        nb_bits[nb_bits.shape[0]-1][ii][i] += d_test_batch.size\n",
    "        nb_errors[nb_errors.shape[0]-1][ii][i] += decoder.evaluate(y_test, d_test_batch, batch_size=test_batch, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db80309d-dc0d-48ef-ba5b-7ac3e19782a5",
   "metadata": {},
   "source": [
    "## Take 16 random elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d255edcd-00ae-4603-bf4b-9ed7019707b7",
   "metadata": {},
   "source": [
    "### Sptit codebook on train / validation parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf85bb-6798-4fe5-950e-bcf0f2952f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 16\n",
    "np.random.seed(seed=1337)\n",
    "idx = list(np.random.choice(2**k, size=train_size, replace=False))\n",
    "print(f'training indices: {idx}')\n",
    "inputs_train = inputs[idx]\n",
    "codewords_train = codewords[idx]\n",
    "d_test = np.delete(inputs, idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b67aab6-6d65-4640-8765-a606a6cda95f",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306bf17-5b49-4a63-b920-0b6eec21a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model, decoder = update_model('dense-128-64-32')\n",
    "history = model.fit(codewords_train, inputs_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                    shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c332b989-32ce-4c6d-bf69-bfb6480a450b",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056fd127-38ab-4aa1-bfc7-94b40599484a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_descr.append('random 16 elements')                                          # Add legend\n",
    "nb_errors = np.append(nb_errors, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "nb_bits = np.append(nb_bits, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "\n",
    "for i in range(0,len(sigmas)):\n",
    "\n",
    "    x_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "    u_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "\n",
    "    u_test[:, log_vector] = d_test\n",
    "    for ii in range(0,d_test.shape[0]):\n",
    "\n",
    "        print(f'Decoding SNR: {i+1}/{len(sigmas)}, CW: {ii+1}/{d_test.shape[0]}   ', end=\"\\r\")\n",
    "\n",
    "        x_test[ii] = polar_transform_iter(u_test[ii])\n",
    "        d_test_batch = np.tile(d_test[ii], (test_batch, 1))\n",
    "        x_test_batch = np.tile(x_test[ii], (test_batch, 1))\n",
    "\n",
    "\n",
    "        # Modulator (BPSK)\n",
    "        s_test = -2*x_test_batch + 1\n",
    "\n",
    "        # Channel (AWGN)\n",
    "        y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "\n",
    "        nb_bits[nb_bits.shape[0]-1][ii][i] += d_test_batch.size\n",
    "        nb_errors[nb_errors.shape[0]-1][ii][i] += decoder.evaluate(y_test, d_test_batch, batch_size=test_batch, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc877656-3527-4c94-bc08-0341bdda5d9c",
   "metadata": {},
   "source": [
    "## Take 32 random elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71514d01-94e3-4f7a-88d6-c863d6aa838e",
   "metadata": {},
   "source": [
    "### Sptit codebook on train / validation parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102acac2-742e-474e-9f1d-b126e8f9f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 32\n",
    "np.random.seed(seed=1337)\n",
    "idx = list(np.random.choice(2**k, size=train_size, replace=False))\n",
    "print(f'training indices: {np.sort(idx)}')\n",
    "inputs_train = inputs[idx]\n",
    "codewords_train = codewords[idx]\n",
    "d_test = np.delete(inputs, idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a0293e-22cd-4aac-a3af-b70fafb2cae0",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a14ce-857d-421d-a740-95aa6bcc5b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model, decoder = update_model('dense-128-64-32')\n",
    "history = model.fit(codewords_train, inputs_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                    shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c41cedf-2c92-489a-b21e-935045a5a84e",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75254e4-390c-4547-ba67-0f53a64824d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_descr.append('random 32 elements')                                          # Add legend\n",
    "nb_errors = np.append(nb_errors, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "nb_bits = np.append(nb_bits, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "\n",
    "for i in range(0,len(sigmas)):\n",
    "\n",
    "    x_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "    u_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "\n",
    "    u_test[:, log_vector] = d_test\n",
    "    for ii in range(0,d_test.shape[0]):\n",
    "\n",
    "        print(f'Decoding SNR: {i+1}/{len(sigmas)}, CW: {ii+1}/{d_test.shape[0]}   ', end=\"\\r\")\n",
    "\n",
    "        x_test[ii] = polar_transform_iter(u_test[ii])\n",
    "        d_test_batch = np.tile(d_test[ii], (test_batch, 1))\n",
    "        x_test_batch = np.tile(x_test[ii], (test_batch, 1))\n",
    "\n",
    "\n",
    "        # Modulator (BPSK)\n",
    "        s_test = -2*x_test_batch + 1\n",
    "\n",
    "        # Channel (AWGN)\n",
    "        y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "\n",
    "        nb_bits[nb_bits.shape[0]-1][ii][i] += d_test_batch.size\n",
    "        nb_errors[nb_errors.shape[0]-1][ii][i] += decoder.evaluate(y_test, d_test_batch, batch_size=test_batch, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6ab45-577a-4331-a3d3-3561a6c5be0f",
   "metadata": {},
   "source": [
    "## Take 64 random elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e88137-b709-4e8e-8872-d4c45ba8a973",
   "metadata": {},
   "source": [
    "### Sptit codebook on train / validation parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624de33-c8ec-410c-9b27-42556e34c13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 64\n",
    "np.random.seed(seed=1337)\n",
    "idx = list(np.random.choice(2**k, size=train_size, replace=False))\n",
    "print(f'training indices: {idx}')\n",
    "inputs_train = inputs[idx]\n",
    "codewords_train = codewords[idx]\n",
    "d_test = np.delete(inputs, idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91616091-2028-481c-8fd4-28deeb893da6",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b1ea4-395d-4ee3-a294-a05044435c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model, decoder = update_model('dense-128-64-32')\n",
    "history = model.fit(codewords_train, inputs_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                    shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d626e7d-24d2-40cd-984a-620c7979e163",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc31c32-94d0-4053-bf11-e8e3cc338ea5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_descr.append('random 64 elements')                                          # Add legend\n",
    "nb_errors = np.append(nb_errors, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "nb_bits = np.append(nb_bits, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "\n",
    "for i in range(0,len(sigmas)):\n",
    "\n",
    "    x_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "    u_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "\n",
    "    u_test[:, log_vector] = d_test\n",
    "    for ii in range(0,d_test.shape[0]):\n",
    "\n",
    "        print(f'Decoding SNR: {i+1}/{len(sigmas)}, CW: {ii+1}/{d_test.shape[0]}   ', end=\"\\r\")\n",
    "\n",
    "        x_test[ii] = polar_transform_iter(u_test[ii])\n",
    "        d_test_batch = np.tile(d_test[ii], (test_batch, 1))\n",
    "        x_test_batch = np.tile(x_test[ii], (test_batch, 1))\n",
    "\n",
    "\n",
    "        # Modulator (BPSK)\n",
    "        s_test = -2*x_test_batch + 1\n",
    "\n",
    "        # Channel (AWGN)\n",
    "        y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "\n",
    "        nb_bits[nb_bits.shape[0]-1][ii][i] += d_test_batch.size\n",
    "        nb_errors[nb_errors.shape[0]-1][ii][i] += decoder.evaluate(y_test, d_test_batch, batch_size=test_batch, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfafaad9-64d3-424f-a9e2-a2983d03bdd9",
   "metadata": {},
   "source": [
    "## Take 128 random elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac0b36-2ce7-4fcd-a676-7207c4682c3f",
   "metadata": {},
   "source": [
    "### Sptit codebook on train / validation parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb34ed6-80ea-4f7a-9203-2e7c8d5f6728",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 128\n",
    "np.random.seed(seed=1337)\n",
    "idx = list(np.random.choice(2**k, size=train_size, replace=False))\n",
    "print(f'training indices: {idx}')\n",
    "inputs_train = inputs[idx]\n",
    "codewords_train = codewords[idx]\n",
    "d_test = np.delete(inputs, idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa5fe17-493d-45c2-be79-0e46ba4e8fce",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d763a03-c91a-4dcf-a5ae-672a47b4da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model, decoder = update_model('dense-128-64-32')\n",
    "history = model.fit(codewords_train, inputs_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                    shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1155d8-b75a-4ac6-bca0-ac5655b6faad",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb5c4d1-f06a-4a4a-bc56-9f7f0919993e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_descr.append('random 128 elements')                                          # Add legend\n",
    "nb_errors = np.append(nb_errors, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "nb_bits = np.append(nb_bits, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "\n",
    "for i in range(0,len(sigmas)):\n",
    "\n",
    "    x_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "    u_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "\n",
    "    u_test[:, log_vector] = d_test\n",
    "    for ii in range(0,d_test.shape[0]):\n",
    "\n",
    "        print(f'Decoding SNR: {i+1}/{len(sigmas)}, CW: {ii+1}/{d_test.shape[0]}   ', end=\"\\r\")\n",
    "\n",
    "        x_test[ii] = polar_transform_iter(u_test[ii])\n",
    "        d_test_batch = np.tile(d_test[ii], (test_batch, 1))\n",
    "        x_test_batch = np.tile(x_test[ii], (test_batch, 1))\n",
    "\n",
    "\n",
    "        # Modulator (BPSK)\n",
    "        s_test = -2*x_test_batch + 1\n",
    "\n",
    "        # Channel (AWGN)\n",
    "        y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "\n",
    "        nb_bits[nb_bits.shape[0]-1][ii][i] += d_test_batch.size\n",
    "        nb_errors[nb_errors.shape[0]-1][ii][i] += decoder.evaluate(y_test, d_test_batch, batch_size=test_batch, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61767e6e-3f51-4f47-8a2f-99e887a66ea0",
   "metadata": {},
   "source": [
    "## Take 86 random elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa7ceb-9353-477a-b395-732c9d49de1e",
   "metadata": {},
   "source": [
    "### Sptit codebook on train / validation parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e0808b-5c71-4f98-93ce-e0802ca8b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 86\n",
    "np.random.seed(seed=1337)\n",
    "idx = list(np.random.choice(2**k, size=train_size, replace=False))\n",
    "print(f'training indices: {idx}')\n",
    "inputs_train = inputs[idx]\n",
    "codewords_train = codewords[idx]\n",
    "d_test = np.delete(inputs, idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b148e92-f714-4f25-930e-7b4365fba6bc",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d85b6-7971-42b0-8961-7f4453081d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model, decoder = update_model('dense-128-64-32')\n",
    "history = model.fit(codewords_train, inputs_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                    shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9a8c4-75ba-44bf-a261-be34fd992afc",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25626332-9607-41b6-bd86-32a008fe32a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_descr.append('random 86 elements')                                          # Add legend\n",
    "nb_errors = np.append(nb_errors, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "nb_bits = np.append(nb_bits, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "\n",
    "for i in range(0,len(sigmas)):\n",
    "\n",
    "    x_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "    u_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "\n",
    "    u_test[:, log_vector] = d_test\n",
    "    for ii in range(0,d_test.shape[0]):\n",
    "\n",
    "        print(f'Decoding SNR: {i+1}/{len(sigmas)}, CW: {ii+1}/{d_test.shape[0]}   ', end=\"\\r\")\n",
    "\n",
    "        x_test[ii] = polar_transform_iter(u_test[ii])\n",
    "        d_test_batch = np.tile(d_test[ii], (test_batch, 1))\n",
    "        x_test_batch = np.tile(x_test[ii], (test_batch, 1))\n",
    "\n",
    "\n",
    "        # Modulator (BPSK)\n",
    "        s_test = -2*x_test_batch + 1\n",
    "\n",
    "        # Channel (AWGN)\n",
    "        y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "\n",
    "        nb_bits[nb_bits.shape[0]-1][ii][i] += d_test_batch.size\n",
    "        nb_errors[nb_errors.shape[0]-1][ii][i] += decoder.evaluate(y_test, d_test_batch, batch_size=test_batch, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7692bb2-d891-4289-a18f-0897b65bd9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 8))\n",
    "for experiment in range(0, 2):\n",
    "    plt.plot(nb_errors[experiment,:,-1]/nb_bits[experiment,:,-1])\n",
    "plt.legend(exp_descr, loc=3)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('codeword index')\n",
    "plt.ylabel('BER')    \n",
    "plt.grid(True)\n",
    "plt.title(f'{code} codes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c36aeca-ec09-4321-96cc-97bf85a691a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ber = np.sum(nb_errors, axis=1) / np.sum(nb_bits, axis=1)\n",
    "\n",
    "plt.figure(figsize = (16, 8))\n",
    "for experiment in range(0, nb_bits.shape[0]):\n",
    "    plt.plot(10*np.log10(1/(2*sigmas**2)) - 10*np.log10(k/N), ber[experiment])\n",
    "plt.legend(exp_descr, loc=3)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('$E_b/N_0$')\n",
    "plt.ylabel('BER')    \n",
    "plt.grid(True)\n",
    "plt.title(f'{code} codes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a334f-13f5-4d32-8f6b-51a053ad63b3",
   "metadata": {},
   "source": [
    "# Iterative picking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc58cd-376a-429b-aa85-f853bf90f05f",
   "metadata": {},
   "source": [
    "### With noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8cc4c-11d2-40f4-b552-c223cc911cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad881212-eec6-4671-9acd-9109a5be79cc",
   "metadata": {},
   "source": [
    "## Train on 16 codewords, test on 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b86f65-1c0d-42df-8799-a26ab609ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 16\n",
    "idx = []\n",
    "for i in range(train_size):\n",
    "    idx.append(i*train_size+i)\n",
    "print(f'training indices: {idx}')\n",
    "inputs_train = inputs[idx]\n",
    "codewords_train = codewords[idx]\n",
    "d_test = np.delete(inputs, idx, axis=0)\n",
    "print(f'training information words:\\n{inputs_train.astype(int)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba30f4b8-c119-4253-b184-7a2116325b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs_train.shape,  d_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3960ec5-b23f-410f-93a9-6b3dca6ea5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b5710b-8d6b-406c-b61b-f472e0c9ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "codewords_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ec0f9-3630-4cad-92ad-9e264007a047",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da03e8-e89c-4187-a5be-6322b35c7b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model, decoder = update_model('dense-128-64-32')\n",
    "history = model.fit(codewords_train, inputs_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                    shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d1c133-8098-45a7-8fdc-0e2399ddf584",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b93ac-0f52-4423-ab1f-179d28752dff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_descr.append('iterative picking, 16 train / 240 test')                     # Add legend\n",
    "nb_errors = np.append(nb_errors, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "nb_bits = np.append(nb_bits, np.zeros((1, 2**k, len(sigmas))), axis=0)         # Add row for new data\n",
    "\n",
    "for i in range(0,len(sigmas)):\n",
    "\n",
    "    x_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "    u_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "\n",
    "    u_test[:, log_vector] = d_test\n",
    "    for ii in range(0,d_test.shape[0]):\n",
    "\n",
    "        print(f'Decoding SNR: {i+1}/{len(sigmas)}, CW: {ii+1}/{d_test.shape[0]}   ', end=\"\\r\")\n",
    "\n",
    "        x_test[ii] = polar_transform_iter(u_test[ii])\n",
    "        d_test_batch = np.tile(d_test[ii], (test_batch, 1))\n",
    "        x_test_batch = np.tile(x_test[ii], (test_batch, 1))\n",
    "\n",
    "\n",
    "        # Modulator (BPSK)\n",
    "        s_test = -2*x_test_batch + 1\n",
    "\n",
    "        # Channel (AWGN)\n",
    "        y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "\n",
    "        nb_bits[nb_bits.shape[0]-1][ii][i] += d_test_batch.size\n",
    "        nb_errors[nb_errors.shape[0]-1][ii][i] += decoder.evaluate(y_test, d_test_batch, batch_size=test_batch, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad21107-dc0a-470b-9804-627c95aa8c05",
   "metadata": {},
   "source": [
    "## Train on 29 codewords, test on 227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1b0c1-2b1f-483a-a2a9-0148cf0c2d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_step_size = 8\n",
    "idx = []\n",
    "for i in range(29):\n",
    "    idx.append(int(i*init_step_size+i))\n",
    "print(f'training indices: {idx}')\n",
    "inputs_train = inputs[idx]\n",
    "codewords_train = codewords[idx]\n",
    "d_test = np.delete(inputs, idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3c4689-2ce3-49c7-9ce1-44c53c84434a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs_train.shape,  d_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b072407-c883-42ba-9b7a-b0528eb9cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d81ed4-893a-4c04-84d0-08a5d3ae4500",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05803b5a-ae99-4bfa-80e8-3e657996a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model, decoder = update_model('dense-128-64-32')\n",
    "history = model.fit(codewords_train, inputs_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                    shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12492329-2146-4835-b1bd-b7ab307fa557",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc962cbf-bb95-4720-8ba3-b6895bf3ea8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_descr.append('iterative picking, 29 train / 227 test')                     # Add legend\n",
    "nb_errors = np.append(nb_errors, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "nb_bits = np.append(nb_bits, np.zeros((1, 2**k, len(sigmas))), axis=0)         # Add row for new data\n",
    "\n",
    "for i in range(0,len(sigmas)):\n",
    "\n",
    "    x_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "    u_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "\n",
    "    u_test[:, log_vector] = d_test\n",
    "    for ii in range(0,d_test.shape[0]):\n",
    "\n",
    "        print(f'Decoding SNR: {i+1}/{len(sigmas)}, CW: {ii+1}/{d_test.shape[0]}   ', end=\"\\r\")\n",
    "\n",
    "        x_test[ii] = polar_transform_iter(u_test[ii])\n",
    "        d_test_batch = np.tile(d_test[ii], (test_batch, 1))\n",
    "        x_test_batch = np.tile(x_test[ii], (test_batch, 1))\n",
    "\n",
    "\n",
    "        # Modulator (BPSK)\n",
    "        s_test = -2*x_test_batch + 1\n",
    "\n",
    "        # Channel (AWGN)\n",
    "        y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "\n",
    "        nb_bits[nb_bits.shape[0]-1][ii][i] += d_test_batch.size\n",
    "        nb_errors[nb_errors.shape[0]-1][ii][i] += decoder.evaluate(y_test, d_test_batch, batch_size=test_batch, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8492ad-a79c-43c2-921b-e67e5f416db4",
   "metadata": {},
   "source": [
    "## Train on 52 codewords, test on 204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea7ffb6-bcf2-4f59-8b12-30a1ad86c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_step_size = 4\n",
    "idx = []\n",
    "for i in range(52):\n",
    "    idx.append(int(i*init_step_size+i))\n",
    "print(f'training indices: {idx}')\n",
    "inputs_train = inputs[idx]\n",
    "codewords_train = codewords[idx]\n",
    "d_test = np.delete(inputs, idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a81f20-6054-41a7-8ee6-ed0cbb653854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs_train.shape, d_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e337763-ea65-4b1c-b31f-96de1ec760b1",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7981cc4f-f160-4121-9f5b-030337052742",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model, decoder = update_model('dense-128-64-32')\n",
    "history = model.fit(codewords_train, inputs_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                    shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca6ff2-d1e4-4dfd-9833-5c5af4e5b289",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9275babd-10bd-47b4-b3c9-981669136137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_descr.append('iterative picking, 52 train / 204 test')                     # Add legend\n",
    "nb_errors = np.append(nb_errors, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "nb_bits = np.append(nb_bits, np.zeros((1, 2**k, len(sigmas))), axis=0)         # Add row for new data\n",
    "\n",
    "for i in range(0,len(sigmas)):\n",
    "\n",
    "    x_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "    u_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "\n",
    "    u_test[:, log_vector] = d_test\n",
    "    for ii in range(0,d_test.shape[0]):\n",
    "\n",
    "        print(f'Decoding SNR: {i+1}/{len(sigmas)}, CW: {ii+1}/{d_test.shape[0]}   ', end=\"\\r\")\n",
    "\n",
    "        x_test[ii] = polar_transform_iter(u_test[ii])\n",
    "        d_test_batch = np.tile(d_test[ii], (test_batch, 1))\n",
    "        x_test_batch = np.tile(x_test[ii], (test_batch, 1))\n",
    "\n",
    "\n",
    "        # Modulator (BPSK)\n",
    "        s_test = -2*x_test_batch + 1\n",
    "\n",
    "        # Channel (AWGN)\n",
    "        y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "\n",
    "        nb_bits[nb_bits.shape[0]-1][ii][i] += d_test_batch.size\n",
    "        nb_errors[nb_errors.shape[0]-1][ii][i] += decoder.evaluate(y_test, d_test_batch, batch_size=test_batch, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d220435-777c-4d14-9fcb-edead59b346c",
   "metadata": {},
   "source": [
    "## Train on 86 codewords, test on 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1cda6f-683b-4dbd-a5ca-92d521d77f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_step_size = 2\n",
    "idx = []\n",
    "for i in range(86):\n",
    "    idx.append(int(i*init_step_size+i))\n",
    "print(f'training indices: {idx}')\n",
    "inputs_train = inputs[idx]\n",
    "codewords_train = codewords[idx]\n",
    "d_test = np.delete(inputs, idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56e581-950e-4ab4-85f0-346f6fea8ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train.shape, d_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6013a4b3-bf99-4594-abc1-ea492960e46f",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7896135-72cd-4ab6-8937-24a1ff46b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model, decoder = update_model('dense-128-64-32')\n",
    "history = model.fit(codewords_train, inputs_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                    shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f062abc2-262f-43ab-ae6e-c885bc16c896",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7effb-8bfb-453f-b960-433527a24466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_descr.append('iterative picking, 86 train / 170 test')                     # Add legend\n",
    "nb_errors = np.append(nb_errors, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "nb_bits = np.append(nb_bits, np.zeros((1, 2**k, len(sigmas))), axis=0)         # Add row for new data\n",
    "\n",
    "for i in range(0,len(sigmas)):\n",
    "\n",
    "    x_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "    u_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "\n",
    "    u_test[:, log_vector] = d_test\n",
    "    for ii in range(0,d_test.shape[0]):\n",
    "\n",
    "        print(f'Decoding SNR: {i+1}/{len(sigmas)}, CW: {ii+1}/{d_test.shape[0]}   ', end=\"\\r\")\n",
    "\n",
    "        x_test[ii] = polar_transform_iter(u_test[ii])\n",
    "        d_test_batch = np.tile(d_test[ii], (test_batch, 1))\n",
    "        x_test_batch = np.tile(x_test[ii], (test_batch, 1))\n",
    "\n",
    "\n",
    "        # Modulator (BPSK)\n",
    "        s_test = -2*x_test_batch + 1\n",
    "\n",
    "        # Channel (AWGN)\n",
    "        y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "\n",
    "        nb_bits[nb_bits.shape[0]-1][ii][i] += d_test_batch.size\n",
    "        nb_errors[nb_errors.shape[0]-1][ii][i] += decoder.evaluate(y_test, d_test_batch, batch_size=test_batch, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f9a91-aa41-4f9c-8b6d-787cd3aedca8",
   "metadata": {},
   "source": [
    "## Train on 30 codewords, test on 226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ba949-4042-41a2-b4c7-5b0da4e7613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_step_size = 8\n",
    "idx = []\n",
    "for i in range(29):\n",
    "    idx.append(int(i*init_step_size+i))\n",
    "print(f'training indices: {idx}')\n",
    "inputs_train = inputs[idx]\n",
    "codewords_train = codewords[idx]\n",
    "d_test = np.delete(inputs, idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db10bec-69e4-443c-8f24-b84a66c7019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train = np.vstack((inputs_train, d_test[-1]))\n",
    "codewords_train = np.vstack((codewords_train, codewords[-1]))\n",
    "d_test = np.delete(d_test, -1, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27748114-3f9d-42d0-a61e-821bb70c8aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs_train.shape, codewords_train.shape, d_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d0227-22cf-4278-9bac-8859c58ad7a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5169e-0491-4aa7-82d3-18c3e61fb513",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model, decoder = update_model('dense-128-64-32')\n",
    "history = model.fit(codewords_train, inputs_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                    shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc17a4-534e-43e2-bdd5-77b5b2ce4bf6",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f93e5b-375b-4df6-b4ec-f69372145154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Same test / train codebook as in 29/227 case, but with last all-true\n",
    "# codewornb_errorsrain CB\n",
    "exp_descr.append('iterative picking, 30 train / 226 test')                     # Add legend\n",
    "nb_errors = np.append(nb_errors, np.zeros((1, 2**k, len(sigmas))), axis=0)     # Add row for new data\n",
    "nb_bits = np.append(nb_bits, np.zeros((1, 2**k, len(sigmas))), axis=0)         # Add row for new data\n",
    "\n",
    "for i in range(0,len(sigmas)):\n",
    "\n",
    "    x_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "    u_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "\n",
    "    u_test[:, log_vector] = d_test\n",
    "    for ii in range(0,d_test.shape[0]):\n",
    "\n",
    "        print(f'Decoding SNR: {i+1}/{len(sigmas)}, CW: {ii+1}/{d_test.shape[0]}   ', end=\"\\r\")\n",
    "\n",
    "        x_test[ii] = polar_transform_iter(u_test[ii])\n",
    "        d_test_batch = np.tile(d_test[ii], (test_batch, 1))\n",
    "        x_test_batch = np.tile(x_test[ii], (test_batch, 1))\n",
    "\n",
    "\n",
    "        # Modulator (BPSK)\n",
    "        s_test = -2*x_test_batch + 1\n",
    "\n",
    "        # Channel (AWGN)\n",
    "        y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "\n",
    "        nb_bits[nb_bits.shape[0]-1][ii][i] += d_test_batch.size\n",
    "        nb_errors[nb_errors.shape[0]-1][ii][i] += decoder.evaluate(y_test, d_test_batch, batch_size=test_batch, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae062a-7d1d-45df-a944-6baa720b0d70",
   "metadata": {},
   "source": [
    "## Plot test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8522b135-e291-4436-9fa2-43be8c942de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92e4a2-ce6e-486e-ace1-c5d3cbf4b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 17})\n",
    "\n",
    "legend = []\n",
    "fig = plt.figure(figsize = (16, 8))\n",
    "for experiment in [1]:\n",
    "    plt.plot(nb_errors[experiment,:,-1]/nb_bits[experiment,:,-1])\n",
    "    legend.append(exp_descr[experiment])\n",
    "plt.vlines([0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240], 0, 1, linestyles='dashed', colors='red')\n",
    "# plt.legend(legend, loc=3)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('codeword index')\n",
    "plt.ylabel('BER')    \n",
    "plt.xlim(0,240)\n",
    "plt.ylim(1.5e-1,1)\n",
    "plt.grid(True)\n",
    "plt.title(f'Polar(16,8). FCNN trained on 16 subsequent cws, validated on unseen cws')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874983e0-0a26-4f8a-9095-d27b12f70f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ber = np.sum(nb_errors, axis=1) / np.sum(nb_bits, axis=1)\n",
    "\n",
    "plt.figure(figsize = (16, 8))\n",
    "for experiment in range(0, len(exp_descr)):\n",
    "    plt.plot(10*np.log10(1/(2*sigmas**2)) - 10*np.log10(k/N), ber[experiment])\n",
    "plt.legend(exp_descr, loc=3)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('$E_b/N_0$')\n",
    "plt.ylabel('BER')    \n",
    "plt.grid(True)\n",
    "plt.title(f'{code} codes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa5f32-f586-4753-9fda-0a1e9a633972",
   "metadata": {},
   "outputs": [],
   "source": [
    "ber = np.sum(nb_errors, axis=1) / np.sum(nb_bits, axis=1)\n",
    "\n",
    "plt.figure(figsize = (16, 8))\n",
    "for experiment in [0, 1, 2, 3]:\n",
    "    plt.plot(10*np.log10(1/(2*sigmas**2)) - 10*np.log10(k/N), ber[experiment])\n",
    "plt.legend(exp_descr, loc=3)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('$E_b/N_0$')\n",
    "plt.ylabel('BER')    \n",
    "plt.grid(True)\n",
    "plt.title(f'{code} codes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a48f6df-1e9f-4e93-ac33-94d7f661f4f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ber = np.sum(nb_errors, axis=1) / np.sum(nb_bits, axis=1)\n",
    "legend = []\n",
    "plt.figure(figsize = (16, 8))\n",
    "for experiment in [0, 1, 2, 3, 5, 6, 7, 8]:\n",
    "    plt.plot(10*np.log10(1/(2*sigmas**2)) - 10*np.log10(k/N), ber[experiment])\n",
    "    legend.append(exp_descr[experiment])\n",
    "plt.legend(legend, loc=3)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('$E_b/N_0$')\n",
    "plt.ylabel('BER')    \n",
    "plt.grid(True)\n",
    "plt.title(f'{code} codes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d2821-4bcf-4d2f-b7f8-7d335be00c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ber = np.sum(nb_errors, axis=1) / np.sum(nb_bits, axis=1)\n",
    "\n",
    "legend = []\n",
    "\n",
    "plt.figure(figsize = (16, 8))\n",
    "for experiment in [9, 10, 11, 13]:\n",
    "    plt.plot(10*np.log10(1/(2*sigmas**2)) - 10*np.log10(k/N), ber[experiment])\n",
    "    legend.append(exp_descr[experiment])\n",
    "plt.legend(legend, loc=3)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('$E_b/N_0$')\n",
    "plt.ylabel('BER')    \n",
    "plt.grid(True)\n",
    "plt.title(f'{code} codes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352f1d6-52fb-4867-8ef1-83634d87439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ber = np.sum(nb_errors, axis=1) / np.sum(nb_bits, axis=1)\n",
    "\n",
    "legend = []\n",
    "\n",
    "fig = plt.figure(figsize = (16, 8))\n",
    "for experiment in [5, 6, 7, -1]:\n",
    "    plt.plot(10*np.log10(1/(2*sigmas**2)) - 10*np.log10(k/N), ber[experiment])\n",
    "    legend.append(exp_descr[experiment])\n",
    "legend = ['Train set - 16 codewords', 'Train set - 32 codewords', 'Train set - 64 codewords', 'Train set - 86 codewords']\n",
    "plt.legend(legend, loc=3)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('$E_b/N_0$')\n",
    "plt.ylabel('BER')    \n",
    "plt.grid(True)\n",
    "# plt.title(f'{code} codes')\n",
    "plt.title(f'Polar(16,8). FCNN trained on random cws, validated on unseen cws')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41eca84-219b-494b-b350-0ff4cafca021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "vscode": {
   "interpreter": {
    "hash": "916a6645d02829d83e24d63305b7c4d404b49fc6597c3c7abca0fa6fe3914033"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
