{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Lambda\n",
    "from keras.layers import LSTM, Embedding, Bidirectional, BatchNormalization, SimpleRNN, Input\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Base parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 8                       # number of information bits\n",
    "N = 16                      # code length\n",
    "\n",
    "nb_epoch = 2**14 \n",
    "batch_size = 256            # size of batches for calculation the gradient\n",
    "LLR = False                 # 'True' enables the log-likelihood-ratio layer\n",
    "optimizer = 'adam'           \n",
    "loss = 'mse'                # or 'binary_crossentropy'\n",
    "# arch = ['lstm-256-128-64-32', 'lstm-128-64-32', 'lstm-64-32',\n",
    "#         'lstm-256', 'lstm-128', 'lstm-64', 'lstm-32', \n",
    "#         'dense-128-64-32', 'dense-256-128-64-32']\n",
    "arch = ['lstm-256', 'lstm-128', 'lstm-64', 'lstm-32', 'dense-256-128-64-32', 'dense-128-64-32']\n",
    "\n",
    "train_SNR_Eb = 1            # training-Eb/No\n",
    "train_SNR_Es = train_SNR_Eb + 10*np.log10(k/N)\n",
    "train_sigma = np.sqrt(1/(2*10**(train_SNR_Es/10))).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modulateBPSK(x):\n",
    "    return -2*x +1;\n",
    "\n",
    "def addNoise(x, sigma):\n",
    "    w = K.random_normal(K.shape(x), mean=0.0, stddev=sigma)        # std was changed to stddev \n",
    "    return x + w\n",
    "\n",
    "def ber(y_true, y_pred):\n",
    "    return K.mean(K.not_equal(y_true, K.round(y_pred)))    \n",
    "\n",
    "def return_output_shape(input_shape):  \n",
    "    return input_shape\n",
    "\n",
    "def compose_model(layers):\n",
    "    model = Sequential()\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "    return model\n",
    "\n",
    "def log_likelihood_ratio(x, sigma):   \n",
    "    return 2*x/np.float32(sigma**2)\n",
    "\n",
    "def errors(y_true, y_pred):\n",
    "    return K.sum(int(K.not_equal(y_true, K.round(y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def half_adder(a,b):\n",
    "    s = a ^ b\n",
    "    c = a & b\n",
    "    return s,c\n",
    "\n",
    "def full_adder(a,b,c):\n",
    "    s = (a ^ b) ^ c\n",
    "    c = (a & b) | (c & (a ^ b))\n",
    "    return s,c\n",
    "\n",
    "def add_bool(a,b):\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError('arrays with different length')\n",
    "    k = len(a)\n",
    "    s = np.zeros(k,dtype=bool)\n",
    "    c = False\n",
    "    for i in reversed(range(0,k)):\n",
    "        s[i], c = full_adder(a[i],b[i],c)    \n",
    "    if c:\n",
    "        warnings.warn(\"Addition overflow!\")\n",
    "    return s\n",
    "\n",
    "def inc_bool(a):\n",
    "    k = len(a)\n",
    "    increment = np.hstack((np.zeros(k-1,dtype=bool), np.ones(1,dtype=bool)))\n",
    "    a = add_bool(a,increment)\n",
    "    return a\n",
    "\n",
    "def bitrevorder(x):\n",
    "    m = np.amax(x)\n",
    "    n = np.ceil(np.log2(m)).astype(int)\n",
    "    for i in range(0,len(x)):\n",
    "        x[i] = int('{:0{n}b}'.format(x[i],n=n)[::-1],2)  \n",
    "    return x\n",
    "\n",
    "def int2bin(x,N):\n",
    "    if isinstance(x, list) or isinstance(x, np.ndarray):\n",
    "        binary = np.zeros((len(x),N),dtype='bool')\n",
    "        for i in range(0,len(x)):\n",
    "            binary[i] = np.array([int(j) for j in bin(x[i])[2:].zfill(N)])\n",
    "    else:\n",
    "        binary = np.array([int(j) for j in bin(x)[2:].zfill(N)],dtype=bool)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def bin2int(b):\n",
    "    if isinstance(b[0], list):\n",
    "        integer = np.zeros((len(b),),dtype=int)\n",
    "        for i in range(0,len(b)):\n",
    "            out = 0\n",
    "            for bit in b[i]:\n",
    "                out = (out << 1) | bit\n",
    "            integer[i] = out\n",
    "    elif isinstance(b, np.ndarray):\n",
    "        if len(b.shape) == 1:\n",
    "            out = 0\n",
    "            for bit in b:\n",
    "                out = (out << 1) | bit\n",
    "            integer = out     \n",
    "        else:\n",
    "            integer = np.zeros((b.shape[0],),dtype=int)\n",
    "            for i in range(0,b.shape[0]):\n",
    "                out = 0\n",
    "                for bit in b[i]:\n",
    "                    out = (out << 1) | bit\n",
    "                integer[i] = out\n",
    "        \n",
    "    return integer\n",
    "\n",
    "def polar_design_awgn(N, k, design_snr_dB):  \n",
    "        \n",
    "    S = 10**(design_snr_dB/10)\n",
    "    z0 = np.zeros(N)\n",
    "\n",
    "    z0[0] = np.exp(-S)\n",
    "    for j in range(1,int(np.log2(N))+1):\n",
    "        u = 2**j\n",
    "        for t in range(0,int(u/2)):\n",
    "            T = z0[t]\n",
    "            z0[t] = 2*T - T**2     # upper channel\n",
    "            z0[int(u/2)+t] = T**2  # lower channel\n",
    "        \n",
    "    # sort into increasing order\n",
    "    idx = np.argsort(z0)\n",
    "        \n",
    "    # select k best channels\n",
    "    idx = np.sort(bitrevorder(idx[0:k]))\n",
    "    \n",
    "    A = np.zeros(N, dtype=bool)\n",
    "    A[idx] = True\n",
    "        \n",
    "    return A\n",
    "\n",
    "def polar_transform_iter(u):\n",
    "\n",
    "    N = len(u)\n",
    "    n = 1\n",
    "    x = np.copy(u)\n",
    "    stages = np.log2(N).astype(int)\n",
    "    for s in range(0,stages):\n",
    "        i = 0\n",
    "        while i < N:\n",
    "            for j in range(0,n):\n",
    "                idx = i+j\n",
    "                x[idx] = x[idx] ^ x[idx+n]\n",
    "            i=i+2*n\n",
    "        n=2*n\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_words(code):\n",
    "    # Create all possible information words\n",
    "    d = np.zeros((2**k,k),dtype=bool)\n",
    "    for i in range(1,2**k):\n",
    "        d[i]= inc_bool(d[i-1])\n",
    "\n",
    "    # Create sets of all possible codewords (codebook)\n",
    "    if code == 'polar':   \n",
    "        # TODO vertical dimention = batch sizes\n",
    "        A = polar_design_awgn(N, k, design_snr_dB=0)  # logical vector indicating the nonfrozen bit locations \n",
    "        x = np.zeros((2**k, N),dtype=bool)\n",
    "        u = np.zeros((2**k, N),dtype=bool)\n",
    "        u[:,A] = d\n",
    "\n",
    "        for i in range(0,2**k):\n",
    "            x[i] = polar_transform_iter(u[i])\n",
    "        return x, d, A\n",
    "\n",
    "    elif code == 'random':\n",
    "\n",
    "        np.random.seed(4267)   # for a 16bit Random Code (r=0.5) with Hamming distance >= 2\n",
    "        x = np.random.randint(0,2,size=(2**k,N), dtype=bool)\n",
    "        return x, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define best train SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape_test = (0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_codebook(codebook_not_real_one):\n",
    "    global N\n",
    "    global output_shape_test\n",
    "    output_shape_test = tf.reshape(codebook_not_real_one, (-1, N, 1))\n",
    "    return output_shape_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def update_model(current_arch):\n",
    "    # Define modulator\n",
    "    modulator_layers = [Lambda(modulateBPSK, \n",
    "                              input_shape=(N,), output_shape=return_output_shape, name=\"modulator\")]\n",
    "    modulator = compose_model(modulator_layers)\n",
    "    modulator.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    # Define noise\n",
    "    noise_layers = [Lambda(addNoise, arguments={'sigma':train_sigma}, \n",
    "                           input_shape=(N,), output_shape=return_output_shape, name=\"noise\")]\n",
    "    noise = compose_model(noise_layers)\n",
    "    noise.compile(optimizer=optimizer, loss=loss)\n",
    "    \n",
    "    #Reshape layer\n",
    "    reshape_layer = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape')]\n",
    "    reshape = compose_model(reshape_layer)\n",
    "    reshape.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    # Define LLR\n",
    "    llr_layers = [Lambda(log_likelihood_ratio, arguments={'sigma':train_sigma}, \n",
    "                         input_shape=(N,), output_shape=return_output_shape, name=\"LLR\")]\n",
    "    llr = compose_model(llr_layers)\n",
    "    llr.compile(optimizer=optimizer, loss=loss)\n",
    "    \n",
    "    \n",
    "    # Define decoder\n",
    "    \n",
    "    if current_arch == 'dense-128-64-32':\n",
    "        decoder_layers = [Dense(128, activation='relu'),\n",
    "                          Dense(64, activation='relu'),\n",
    "                          Dense(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'dense-256-128-64-32':\n",
    "        decoder_layers = [Dense(256, activation='relu', input_shape=(N,)),\n",
    "                          Dense(128, activation='relu'),\n",
    "                          Dense(64, activation='relu'),\n",
    "                          Dense(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'lstm-32':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'lstm-64':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'lstm-128':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'lstm-256':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "        \n",
    "    elif current_arch == 'lstm-64-32':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(64, activation='relu'), \n",
    "                          Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape_2'),\n",
    "                          LSTM(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "        \n",
    "    elif current_arch == 'lstm-128-64-32':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(128, activation='relu'), \n",
    "                          Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape_2'),\n",
    "                          LSTM(64, activation='relu'), \n",
    "                          Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape_3'),\n",
    "                          LSTM(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'lstm-256-128-64-32':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(64, activation='relu', recurrent_dropout=0.5),\n",
    "                          Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape_2'),\n",
    "                          LSTM(64, activation='relu', recurrent_dropout=0.5),\n",
    "                          Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape_3'),\n",
    "                          LSTM(64, activation='relu', recurrent_dropout=0.5), \n",
    "                          Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape_4'),\n",
    "                          LSTM(64, activation='relu', recurrent_dropout=0.5),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "\n",
    "\n",
    "    # Define model\n",
    "    if LLR:\n",
    "        model_layers = modulator_layers + noise_layers  + llr_layers + decoder_layers\n",
    "    else:\n",
    "        model_layers = modulator_layers + noise_layers  + decoder_layers\n",
    "#         model_layers = decoder_layers\n",
    "    model = compose_model(model_layers)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[ber])\n",
    "    return model, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Polar codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = 'polar'              # type of code ('random' or 'polar')\n",
    "codewords, inputs, log_vector = create_words(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_batch = 10000  \n",
    "num_words = 1000000      # multiple of test_batch\n",
    "\n",
    "SNR_dB_start_Eb = 0\n",
    "SNR_dB_stop_Eb = 5\n",
    "SNR_points = 20\n",
    "\n",
    "SNR_dB_start_Es = SNR_dB_start_Eb + 10*np.log10(k/N)\n",
    "SNR_dB_stop_Es = SNR_dB_stop_Eb + 10*np.log10(k/N)\n",
    "\n",
    "sigma_start = np.sqrt(1/(2*10**(SNR_dB_start_Es/10)))\n",
    "sigma_stop = np.sqrt(1/(2*10**(SNR_dB_stop_Es/10)))\n",
    "sigmas = np.linspace(sigma_start, sigma_stop, SNR_points)\n",
    "\n",
    "nb_errors = np.zeros((len(arch),len(sigmas)),dtype=int)\n",
    "nb_bits = np.zeros((len(arch),len(sigmas)),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 of 9\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 809, in train_step\n        loss = self.compiled_loss(\n    File \"/opt/anaconda3/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/anaconda3/lib/python3.8/site-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/anaconda3/lib/python3.8/site-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/anaconda3/lib/python3.8/site-packages/keras/losses.py\", line 1204, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 16384 and 256 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential_58/dense_8/Sigmoid, mean_squared_error/Cast)' with input shapes: [16384,8], [256,8].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\", line 809, in train_step\n        loss = self.compiled_loss(\n    File \"/opt/anaconda3/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/anaconda3/lib/python3.8/site-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/anaconda3/lib/python3.8/site-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/anaconda3/lib/python3.8/site-packages/keras/losses.py\", line 1204, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 16384 and 256 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential_58/dense_8/Sigmoid, mean_squared_error/Cast)' with input shapes: [16384,8], [256,8].\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for current_arch in arch:\n",
    "    model, decoder = update_model(current_arch)\n",
    "    \n",
    "    print(f'fit {arch.index(current_arch)+1} of {len(arch)}')\n",
    "    history = model.fit(codewords, inputs, batch_size=batch_size, epochs=nb_epoch, verbose=0, shuffle=True)\n",
    "\n",
    "\n",
    "    print(f'decode {arch.index(current_arch)+1} of {len(arch)}')\n",
    "    for i in range(0,len(sigmas)):\n",
    "\n",
    "        for ii in range(0,np.round(num_words/test_batch).astype(int)):\n",
    "\n",
    "            # Source\n",
    "            np.random.seed(0)\n",
    "            d_test = np.random.randint(0,2,size=(test_batch,k)) \n",
    "\n",
    "            # Encoder\n",
    "            if code == 'polar':\n",
    "                x_test = np.zeros((test_batch, N),dtype=bool)\n",
    "                u_test = np.zeros((test_batch, N),dtype=bool)\n",
    "                u_test[:,log_vector] = d_test\n",
    "\n",
    "                for iii in range(0,test_batch):\n",
    "                    x_test[iii] = polar_transform_iter(u_test[iii])\n",
    "\n",
    "            elif code == 'random':\n",
    "                x_test = np.zeros((test_batch, N),dtype=bool)\n",
    "                for iii in range(0,test_batch):\n",
    "                    x_test[iii] = codewords[bin2int(d_test[iii])]\n",
    "\n",
    "            # Modulator (BPSK)\n",
    "            s_test = -2*x_test + 1\n",
    "\n",
    "            # Channel (AWGN)\n",
    "            y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "            if LLR:\n",
    "                y_test = 2*y_test/(sigmas[i]**2)\n",
    "\n",
    "            # Decoder\n",
    "            nb_bits[arch.index(current_arch)][i] += d_test.size\n",
    "            nb_errors[arch.index(current_arch)][i] += decoder.evaluate(y_test, d_test, batch_size=test_batch, verbose=0)[1]\n",
    "    \n",
    "            np.savetxt(f'eval_nn_arch/{code}/bits_{code}_{N}_{k}_{arch}.out', nb_bits, delimiter=', ')\n",
    "            np.savetxt(f'eval_nn_arch/{code}/errors_{code}_{N}_{k}_{arch}.out', nb_errors, delimiter=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "eval_nn_arch/polar/bits_polar_16_8_['lstm-256', 'lstm-128', 'lstm-64', 'lstm-32', 'dense-128-64-32', 'dense-256-128-64-32'].out not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bfdeee005617>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnb_errors_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnb_bits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'eval_nn_arch/{code}/bits_{code}_{N}_{k}_{arch}.out'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m', '\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mnb_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'eval_nn_arch/{code}/errors_{code}_{N}_{k}_{arch}.out'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m', '\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    529\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: eval_nn_arch/polar/bits_polar_16_8_['lstm-256', 'lstm-128', 'lstm-64', 'lstm-32', 'dense-128-64-32', 'dense-256-128-64-32'].out not found."
     ]
    }
   ],
   "source": [
    "result_map = np.loadtxt('map/{}/results_{}_map_{}_{}.txt'.format(code,code,N,k), delimiter=', ')\n",
    "sigmas_map = result_map[:,0]\n",
    "nb_bits_map = result_map[:,1]\n",
    "nb_errors_map = result_map[:,2]\n",
    "\n",
    "nb_bits = np.loadtxt(f'eval_nn_arch/{code}/bits_{code}_{N}_{k}_{arch}.out', delimiter=', ' )\n",
    "nb_errors = np.loadtxt(f'eval_nn_arch/{code}/errors_{code}_{N}_{k}_{arch}.out', delimiter=', ' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot Bit-Error-Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "legend = []\n",
    "for current_arch in arch:\n",
    "    plt.plot(10*np.log10(1/(2*sigmas**2)) - 10*np.log10(k/N), nb_errors[arch.index(current_arch)]/nb_bits[arch.index(current_arch)])\n",
    "    legend.append(f'architecture={current_arch}') \n",
    "\n",
    "plt.plot(10*np.log10(1/(2*sigmas_map**2)) - 10*np.log10(k/N), nb_errors_map/nb_bits_map)\n",
    "legend.append('MAP')\n",
    "\n",
    "plt.legend(legend, loc=3)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('$E_b/N_0$')\n",
    "plt.ylabel('BER')    \n",
    "plt.grid(True)\n",
    "plt.title(f'{code} codes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = 'random'              # type of code ('random' or 'polar')\n",
    "codewords, inputs = create_words(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_batch = 10000  \n",
    "num_words = 1000000      # multiple of test_batch\n",
    "\n",
    "SNR_dB_start_Eb = 0\n",
    "SNR_dB_stop_Eb = 5\n",
    "SNR_points = 20\n",
    "\n",
    "SNR_dB_start_Es = SNR_dB_start_Eb + 10*np.log10(k/N)\n",
    "SNR_dB_stop_Es = SNR_dB_stop_Eb + 10*np.log10(k/N)\n",
    "\n",
    "sigma_start = np.sqrt(1/(2*10**(SNR_dB_start_Es/10)))\n",
    "sigma_stop = np.sqrt(1/(2*10**(SNR_dB_stop_Es/10)))\n",
    "sigmas = np.linspace(sigma_start, sigma_stop, SNR_points)\n",
    "\n",
    "nb_errors = np.zeros((len(arch),len(sigmas)),dtype=int)\n",
    "nb_bits = np.zeros((len(arch),len(sigmas)),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit 1 of 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for current_arch in arch:\n",
    "    model, decoder = update_model(current_arch)\n",
    "    \n",
    "    print(f'fit {arch.index(current_arch)+1} of {len(arch)}')\n",
    "    history = model.fit(codewords, inputs, batch_size=batch_size, epochs=nb_epoch, verbose=0, shuffle=True)\n",
    "\n",
    "\n",
    "    print(f'decode {arch.index(current_arch)+1} of {len(arch)}')\n",
    "    for i in range(0,len(sigmas)):\n",
    "\n",
    "        for ii in range(0,np.round(num_words/test_batch).astype(int)):\n",
    "\n",
    "            # Source\n",
    "            np.random.seed(0)\n",
    "            d_test = np.random.randint(0,2,size=(test_batch,k)) \n",
    "\n",
    "            # Encoder\n",
    "            if code == 'polar':\n",
    "                x_test = np.zeros((test_batch, N),dtype=bool)\n",
    "                u_test = np.zeros((test_batch, N),dtype=bool)\n",
    "                u_test[:,log_vector] = d_test\n",
    "\n",
    "                for iii in range(0,test_batch):\n",
    "                    x_test[iii] = polar_transform_iter(u_test[iii])\n",
    "\n",
    "            elif code == 'random':\n",
    "                x_test = np.zeros((test_batch, N),dtype=bool)\n",
    "                for iii in range(0,test_batch):\n",
    "                    x_test[iii] = codewords[bin2int(d_test[iii])]\n",
    "\n",
    "            # Modulator (BPSK)\n",
    "            s_test = -2*x_test + 1\n",
    "\n",
    "            # Channel (AWGN)\n",
    "            y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "            if LLR:\n",
    "                y_test = 2*y_test/(sigmas[i]**2)\n",
    "\n",
    "            # Decoder\n",
    "            nb_bits[arch.index(current_arch)][i] += d_test.size\n",
    "            nb_errors[arch.index(current_arch)][i] += decoder.evaluate(y_test, d_test, batch_size=test_batch, verbose=0)[1]\n",
    "    \n",
    "            np.savetxt(f'eval_nn_arch/{code}/bits_{code}_{N}_{k}_{arch}.out', nb_bits, delimiter=', ')\n",
    "            np.savetxt(f'eval_nn_arch/{code}/errors_{code}_{N}_{k}_{arch}.out', nb_errors, delimiter=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_map = np.loadtxt('map/{}/results_{}_map_{}_{}.txt'.format(code,code,N,k), delimiter=', ')\n",
    "sigmas_map = result_map[:,0]\n",
    "nb_bits_map = result_map[:,1]\n",
    "nb_errors_map = result_map[:,2]\n",
    "\n",
    "nb_bits = np.loadtxt(f'eval_nn_arch/{code}/bits_{code}_{N}_{k}_{arch}.out', delimiter=', ' )\n",
    "nb_errors = np.loadtxt(f'eval_nn_arch/{code}/errors_{code}_{N}_{k}_{arch}.out', delimiter=', ' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot Bit-Error-Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "legend = []\n",
    "for current_arch in arch:\n",
    "    plt.plot(10*np.log10(1/(2*sigmas**2)) - 10*np.log10(k/N), nb_errors[arch.index(current_arch)]/nb_bits[arch.index(current_arch)])\n",
    "    legend.append(f'architecture={current_arch}') \n",
    "\n",
    "plt.plot(10*np.log10(1/(2*sigmas_map**2)) - 10*np.log10(k/N), nb_errors_map/nb_bits_map)\n",
    "legend.append('MAP')\n",
    "\n",
    "plt.legend(legend, loc=3)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('$E_b/N_0$')\n",
    "plt.ylabel('BER')    \n",
    "plt.grid(True)\n",
    "plt.title(f'{code} codes')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
