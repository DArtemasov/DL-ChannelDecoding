{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f60847-a1d0-40e4-a17f-85f147771240",
   "metadata": {},
   "source": [
    "# Create load parity-check and generator matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe61ae47-8feb-4007-b5a9-2fbefb425d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path, sys, bz2\n",
    "import numpy as np\n",
    "import pyldpc as ldpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43df5d9e-0f4e-4e80-8f83-eee5da98cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBinaryMatrix(source):\n",
    "    \"\"\"Creates a binary matrix of type :class:`np.ndarray` from either a file or a\n",
    "    two-dimensional python list.\n",
    "    If `source` is a file path, the file must eitheddr contain an explicit representation of the\n",
    "    matrix (by means of whitespace-separated '0' and '1' characters) or be in the AList format\n",
    "    (see alistToNumpy docstring).\n",
    "    The file may be bzip2'ed, in which case it will be decompressed automatically.\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray[dtype=int]\n",
    "        Numpy ndarray representation of the given matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    source = os.path.expanduser(source)\n",
    "    fileObj = bz2.BZ2File(source, 'r') if source.endswith('bz2') else open(source, 'rt')\n",
    "    with fileObj as f:\n",
    "        lines = [[int(x) for x in l.strip().split()]\n",
    "                  for l in f.readlines()\n",
    "                  if len(l.strip()) > 0]\n",
    "\n",
    "    if lines[0][0] in (0, 1):  # explicit 0/1 representation\n",
    "        return np.array(lines, dtype=np.int)\n",
    "    return alistToNumpy(lines)\n",
    "\n",
    "def alistToNumpy(lines):\n",
    "    \"\"\"Converts a parity-check matrix in AList format to a 0/1 numpy array. The argument is a\n",
    "    list-of-lists corresponding to the lines of the AList format, already parsed to integers\n",
    "    if read from a text file.\n",
    "    The AList format is introduced on http://www.inference.phy.cam.ac.uk/mackay/codes/alist.html.\n",
    "    This method supports a \"reduced\" AList format where lines 3 and 4 (containing column and row\n",
    "    weights, respectively) and the row-based information (last part of the Alist file) are omitted.\n",
    "    Example:\n",
    "        >>> alistToNumpy([[3,2], [2, 2], [1,1,2], [2,2], [1], [2], [1,2], [1,2,3,4]])\n",
    "        array([[1, 0, 1],\n",
    "               [0, 1, 1]])\n",
    "    \"\"\"\n",
    "    nCols, nRows = lines[0]\n",
    "    if len(lines[2]) == nCols and len(lines[3]) == nRows:\n",
    "        startIndex = 4\n",
    "    else:\n",
    "        startIndex = 2\n",
    "    matrix = np.zeros((nRows, nCols), dtype=int)\n",
    "    for col, nonzeros in enumerate(lines[startIndex:startIndex + nCols]):\n",
    "        for rowIndex in nonzeros:\n",
    "            if rowIndex != 0:\n",
    "                matrix[rowIndex - 1, col] = 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15cccd79-899f-4b81-ad13-e41434b67cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = getBinaryMatrix('LDPC/CCSDS_ldpc_n32_k16.alist')\n",
    "G = ldpc.coding_matrix(H, sparse=False).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341b1aa3-8291-488c-960c-0f906b49aa48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether GenMatrix is correctly calculated\n",
    "G @ H.T % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc2f026b-be48-4e3b-889d-901b84c313d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 16\n",
    "N = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76eaf0e8-00d3-404c-9102-6952d0d12e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_adder(a,b,c):\n",
    "    s = (a ^ b) ^ c\n",
    "    c = (a & b) | (c & (a ^ b))\n",
    "    return s,c\n",
    "\n",
    "def add_bool(a,b):\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError('arrays with different length')\n",
    "    k = len(a)\n",
    "    s = np.zeros(k,dtype=bool)\n",
    "    c = False\n",
    "    for i in reversed(range(0,k)):\n",
    "        s[i], c = full_adder(a[i],b[i],c)    \n",
    "    if c:\n",
    "        warnings.warn(\"Addition overflow!\")\n",
    "    return s\n",
    "\n",
    "def inc_bool(a):\n",
    "    k = len(a)\n",
    "    increment = np.hstack((np.zeros(k-1,dtype=bool), np.ones(1,dtype=bool)))\n",
    "    a = add_bool(a,increment)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5196ad83-e4f6-450b-96b7-1ad579389d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.zeros((2**k,k),dtype=bool)\n",
    "for i in range(1,2**k):\n",
    "    inputs[i]= inc_bool(inputs[i-1])\n",
    "codewords = inputs @ G % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5d56f2-d947-4eaf-9571-ad89059bbb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((65536, 16), (65536, 32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, codewords.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aa35e4-02ca-42bb-b665-1aeea26d4532",
   "metadata": {},
   "source": [
    "# Define NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c2e3fd-413e-4ba1-9f0b-ffa49a835082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Lambda\n",
    "from keras.layers import LSTM, Embedding, Bidirectional, BatchNormalization, SimpleRNN, Input, TimeDistributed\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.keras import TqdmCallback\n",
    "from scipy.spatial import distance\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b158710e-98d6-41ae-ae84-19ee376455f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 2**18\n",
    "batch_size = 256            # size of batches for calculation the gradient\n",
    "LLR = False                 # 'True' enables the log-likelihood-ratio layer\n",
    "noise = True\n",
    "optimizer = 'adam'           \n",
    "loss = 'mse'                # or 'binary_crossentropy'\n",
    "\n",
    "train_SNR_Eb = 1            # training-Eb/No\n",
    "train_SNR_Es = train_SNR_Eb + 10*np.log10(k/N)\n",
    "train_sigma = np.sqrt(1/(2*10**(train_SNR_Es/10))).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1939b0c5-5e33-4d0b-9ffd-b0ad4d87ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulateBPSK(x):\n",
    "    return -2*x +1;\n",
    "\n",
    "def addNoise(x, sigma):\n",
    "    w = K.random_normal(K.shape(x), mean=0.0, stddev=sigma)        # std was changed to stddev \n",
    "    return x + w\n",
    "\n",
    "def ber(y_true, y_pred):\n",
    "    return K.mean(K.not_equal(y_true, K.round(y_pred)))    \n",
    "\n",
    "def return_output_shape(input_shape):  \n",
    "    return input_shape\n",
    "\n",
    "def compose_model(layers):\n",
    "    model = Sequential()\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "    return model\n",
    "\n",
    "def log_likelihood_ratio(x, sigma):   \n",
    "    return 2*x/np.float32(sigma**2)\n",
    "\n",
    "def errors(y_true, y_pred):\n",
    "    return K.sum(int(K.not_equal(y_true, K.round(y_pred))))\n",
    "\n",
    "def reshape_codebook(cb):\n",
    "    global N\n",
    "    global output_shape_test\n",
    "    output_shape_test = tf.reshape(cb, (-1, N, 1))\n",
    "    return output_shape_test\n",
    "\n",
    "output_shape_test = (0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d67d5f1-7ba9-4c04-bbac-275dacfb5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(current_arch):\n",
    "    # Define modulator\n",
    "    modulator_layers = [Lambda(modulateBPSK, \n",
    "                              input_shape=(N,), output_shape=return_output_shape, name=\"modulator\")]\n",
    "    modulator = compose_model(modulator_layers)\n",
    "    modulator.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    # Define noise\n",
    "    noise_layers = [Lambda(addNoise, arguments={'sigma':train_sigma}, \n",
    "                           input_shape=(N,), output_shape=return_output_shape, name=\"noise\")]\n",
    "    noise = compose_model(noise_layers)\n",
    "    noise.compile(optimizer=optimizer, loss=loss)\n",
    "    \n",
    "    # Reshape layer\n",
    "    reshape_layer = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape')]\n",
    "    reshape = compose_model(reshape_layer)\n",
    "    reshape.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    # Define LLR\n",
    "    llr_layers = [Lambda(log_likelihood_ratio, arguments={'sigma':train_sigma}, \n",
    "                         input_shape=(N,), output_shape=return_output_shape, name=\"LLR\")]\n",
    "    llr = compose_model(llr_layers)\n",
    "    llr.compile(optimizer=optimizer, loss=loss)\n",
    "    \n",
    "    \n",
    "    # Define decoder\n",
    "    \n",
    "    if current_arch == 'dense-128-64-32':\n",
    "        decoder_layers = [Dense(128, activation='relu'),\n",
    "                          Dense(64, activation='relu'),\n",
    "                          Dense(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'dense-256-128-64-32':\n",
    "        decoder_layers = [Dense(256, activation='relu', input_shape=(N,)),\n",
    "                          Dense(128, activation='relu'),\n",
    "                          Dense(64, activation='relu'),\n",
    "                          Dense(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'lstm-32':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'lstm-64':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(64, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'lstm-128':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(128, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'lstm-256':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(256, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "        \n",
    "    elif current_arch == 'lstm-64-32':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(64, activation='relu', return_sequences=True), \n",
    "                          LSTM(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "        \n",
    "    elif current_arch == 'lstm-128-64-32':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(128, activation='relu', return_sequences=True), \n",
    "                          LSTM(64, activation='relu', return_sequences=True), \n",
    "                          LSTM(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "        \n",
    "    elif current_arch == 'lstm-256-128-64-32':\n",
    "        decoder_layers = [Lambda(reshape_codebook,\n",
    "                            input_shape = (N,), output_shape = output_shape_test, name = 'reshape'),\n",
    "                          LSTM(256, activation='relu', return_sequences=True),\n",
    "                          LSTM(128, activation='relu', return_sequences=True),\n",
    "                          LSTM(64, activation='relu', return_sequences=True), \n",
    "                          LSTM(32, activation='relu'),\n",
    "                          Dense(k, activation='sigmoid')]\n",
    "        decoder = compose_model(decoder_layers)\n",
    "        decoder.compile(optimizer=optimizer, loss=loss, metrics=[errors])\n",
    "\n",
    "\n",
    "    # Define model\n",
    "#     if (LLR and noise):\n",
    "#         model_layers = modulator_layers + noise_layers  + llr_layers + decoder_layers\n",
    "#     elif (not LLR and noise):\n",
    "#         model_layers = modulator_layers + noise_layers + decoder_layers\n",
    "#     elif (not LLR and not noise):\n",
    "#         model_layers = modulator_layers + decoder_layers\n",
    "\n",
    "    if noise:\n",
    "        model_layers = modulator_layers + noise_layers + decoder_layers\n",
    "    else:\n",
    "        model_layers = modulator_layers + decoder_layers\n",
    "    model = compose_model(model_layers)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[ber])\n",
    "    return model, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a4691d-009e-48fa-ab68-aeadcdee1d5c",
   "metadata": {},
   "source": [
    "# Split codebook into train and validation sets (train:49152, val:16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ad8fd00-7ceb-4160-ab26-62e04537e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(codewords.shape[0] * 0.75)\n",
    "np.random.seed(seed=1337)\n",
    "idx = list(np.random.choice(2**k, size=train_size, replace=False))\n",
    "# print(f'training indices: {idx}')\n",
    "inputs_train = inputs[idx]\n",
    "codewords_train = codewords[idx]\n",
    "inputs_test = np.delete(inputs, idx, axis=0)\n",
    "codewords_test = np.delete(codewords, idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44e4eaa7-feb5-4661-98dd-7d287c83b057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49152, 16), (49152, 32), (16384, 16), (16384, 32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train.shape, codewords_train.shape, inputs_test.shape, codewords_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6fb81-b00e-49f2-8db4-a8bd0af000ff",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8b137ad-1653-4f15-b48d-03c3d6320c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040f42f4b6394dbf9000a8e002f2a8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18h 10min 6s, sys: 2h 46min 18s, total: 20h 56min 24s\n",
      "Wall time: 13h 38min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model, decoder = update_model('dense-128-64-32')\n",
    "history = model.fit(codewords_train, inputs_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "                    shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f3c1e-659d-494f-9421-b5a19694d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LDPC/model-ldpc')\n",
    "model.save('LDPC/decoder-ldpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423564ec-86d6-4cf8-9058-86f2a2cacc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = load_model('LDPC/decoder-ldpc', custom_objects={'errors':errors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "189b690f-efbc-4a28-933e-d231476ea0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = 100000\n",
    "\n",
    "SNR_dB_start_Eb = 0\n",
    "SNR_dB_stop_Eb = 10\n",
    "SNR_points = 10\n",
    "\n",
    "SNR_dB_start_Es = SNR_dB_start_Eb + 10*np.log10(k/N)\n",
    "SNR_dB_stop_Es = SNR_dB_stop_Eb + 10*np.log10(k/N)\n",
    "\n",
    "sigma_start = np.sqrt(1/(2*10**(SNR_dB_start_Es/10)))\n",
    "sigma_stop = np.sqrt(1/(2*10**(SNR_dB_stop_Es/10)))\n",
    "sigmas = np.linspace(sigma_start, sigma_stop, SNR_points)\n",
    "\n",
    "nb_errors = np.zeros((1,2**k,len(sigmas)),dtype=int)\n",
    "nb_bits = np.zeros((1,2**k,len(sigmas)),dtype=int)\n",
    "exp_descr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f64dd5-1bda-4407-aaa2-257ee1dba594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding SNR: 7/10, CW: 2068/16384    \r"
     ]
    }
   ],
   "source": [
    "exp_descr.append('LDPC train on 49152, test on 16384 random')                                          # Add legend\n",
    "\n",
    "for i in range(0,len(sigmas)):\n",
    "\n",
    "#     x_test = np.zeros((inputs_test.shape[0], N), dtype=bool)\n",
    "#     u_test = np.zeros((d_test.shape[0], N), dtype=bool)\n",
    "\n",
    "#     u_test[:, log_vector] = d_test\n",
    "    for ii in range(0,codewords_test.shape[0]):\n",
    "\n",
    "        print(f'Decoding SNR: {i+1}/{len(sigmas)}, CW: {ii+1}/{codewords_test.shape[0]}   ', end=\"\\r\")\n",
    "\n",
    "#         x_test[ii] = polar_transform_iter(u_test[ii])\n",
    "        inputs_test_batch = np.tile(inputs_test[ii], (test_batch, 1))\n",
    "        codewords_test_batch = np.tile(codewords_test[ii], (test_batch, 1))\n",
    "\n",
    "\n",
    "        # Modulator (BPSK)\n",
    "        s_test = -2*codewords_test_batch + 1\n",
    "\n",
    "        # Channel (AWGN)\n",
    "        y_test = s_test + sigmas[i]*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "\n",
    "        nb_bits[nb_bits.shape[0]-1][ii][i] += inputs_test_batch.size\n",
    "        nb_errors[nb_errors.shape[0]-1][ii][i] += decoder.evaluate(y_test, inputs_test_batch, batch_size=test_batch, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c079e1b5-95fa-4afc-8afa-6915825f7c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
